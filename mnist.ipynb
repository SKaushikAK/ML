{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43115740-236e-416b-8911-0236af3abfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434795b9-980c-4c7b-9c68-075158d60ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        print(num, rows, cols)\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num, rows*cols)\n",
    "        return images / 255.0\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b66a525-ec25-45dd-8a76-8dd8957c1b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_mnist_images('datasets/MNIST/train-images.idx3-ubyte')\n",
    "y_train = load_mnist_labels('datasets/MNIST/train-labels.idx1-ubyte')\n",
    "X_test = load_mnist_images('datasets/MNIST/t10k-images.idx3-ubyte')\n",
    "y_test = load_mnist_labels('datasets/MNIST/t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44faeeb8-fddc-44b5-a734-bec678f49b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cc132-fd4e-4e53-9305-fd924efb1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 # 28 x 28 pixels\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 64\n",
    "output_size = 10 # numbers from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57cafb-0808-4419-8662-99a3ac26a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"W1\": np.random.randn(input_size, hidden_size1) * np.sqrt(2.0 / input_size),\n",
    "    \"W2\": np.random.randn(hidden_size1, hidden_size2) * np.sqrt(2.0 / hidden_size1),\n",
    "    \"W3\": np.random.randn(hidden_size2, output_size) * np.sqrt(2.0 / hidden_size2),\n",
    "    \"b1\": np.zeros((1, hidden_size1)),\n",
    "    \"b2\": np.zeros((1, hidden_size2)),\n",
    "    \"b3\": np.zeros((1, output_size))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d47351-443e-4a60-bda1-50261ccf3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True)) # numerical stability (removal of very large nums as exponentiation will result in large nums)\n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True) # normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac94b0d-db6a-4036-b6c0-ad0b8833e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, weights):\n",
    "    Z1 = np.dot(X, weights[\"W1\"]) + weights[\"b1\"]\n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    Z2 = np.dot(A1, weights[\"W2\"]) + weights[\"b2\"]\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    Z3 = np.dot(A2, weights[\"W3\"]) + weights[\"b3\"]\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    "    return Z1, A1, Z2, A2, Z3, A3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e36b8-2f0d-484a-a7ce-527ae641ce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y_pred, Y_true, weights, lambda_):\n",
    "    m = Y_true.shape[0]\n",
    "    log_likelihood = -np.log(Y_pred[range(m), Y_true])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "\n",
    "    L2_regularizer = (lambda_ / (2 * m) * (\n",
    "        np.sum(weights[\"W1\"] ** 2) + np.sum(weights[\"W2\"] ** 2) + np.sum(weights[\"W3\"] ** 2)\n",
    "    ))\n",
    "\n",
    "    return loss + L2_regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d73f8e-cbd8-4fc0-a0d4-9a8e39262174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X, Y_true, A1, A2, A3, weights, learning_rate, lambda_):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    Y_one_hot = np.zeros((m, output_size))\n",
    "    Y_one_hot[np.arange(m), Y_true] = 1\n",
    "\n",
    "    dZ3 = A3 - Y_one_hot\n",
    "    dW3 = (np.dot(A2.T, dZ3) + lambda_ * weights[\"W3\"]) / m\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA2 = np.dot(dZ3, weights[\"W3\"].T)\n",
    "    dZ2 = dA2 * (A2 > 0)  # ReLU derivative\n",
    "    dW2 = (np.dot(A1.T, dZ2) + lambda_ * weights[\"W2\"]) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA1 = np.dot(dZ2, weights[\"W2\"].T)\n",
    "    dZ1 = dA1 * (A1 > 0)  # ReLU derivative\n",
    "    dW1 = (np.dot(X.T, dZ1) + lambda_ * weights[\"W1\"]) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "    # Update weights\n",
    "    weights[\"W1\"] -= learning_rate * dW1\n",
    "    weights[\"b1\"] -= learning_rate * db1\n",
    "    weights[\"W2\"] -= learning_rate * dW2\n",
    "    weights[\"b2\"] -= learning_rate * db2\n",
    "    weights[\"W3\"] -= learning_rate * dW3\n",
    "    weights[\"b3\"] -= learning_rate * db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc37abd4-400f-4255-ae89-24f72fc552d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "lambda_ = 0.01 # L2 regularization factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5c5a7-d19e-4ba3-b3f0-9695fe8f1262",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    shuffle_indices = np.random.permutation(X_train.shape[0])\n",
    "    X_train, y_train = X_train[shuffle_indices], y_train[shuffle_indices]\n",
    "\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "        Z1, A1, Z2, A2, Z3, A3 = forward_propagation(X_batch, weights)\n",
    "\n",
    "        learning_rate = 0.01 / (1 + 0.01 * epoch) # Learning rate decay\n",
    "        backpropagation(X_batch, y_batch, A1, A2, A3, weights, learning_rate, lambda_)\n",
    "\n",
    "\n",
    "    _, _, _, _, _, train_pred = forward_propagation(X_train, weights)\n",
    "    train_loss = compute_loss(train_pred, y_train, weights, lambda_)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a4b19-fdcb-445f-80ba-d12ef229f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights):\n",
    "    _, _, _, _, _, A3 = forward_propagation(X, weights)\n",
    "    return np.argmax(A3, axis=1)\n",
    "\n",
    "y_pred = predict(X_test, weights)\n",
    "accuracy = np.mean(y_pred == y_test) * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
